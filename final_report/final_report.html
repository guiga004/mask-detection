<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.302">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mohammed Guiga">

<title>Deep Learning Models for Measuring Mask-Wearing Behavior in Public Spaces</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="final_report_files/libs/clipboard/clipboard.min.js"></script>
<script src="final_report_files/libs/quarto-html/quarto.js"></script>
<script src="final_report_files/libs/quarto-html/popper.min.js"></script>
<script src="final_report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="final_report_files/libs/quarto-html/anchor.min.js"></script>
<link href="final_report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="final_report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="final_report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="final_report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="final_report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deep Learning Models for Measuring Mask-Wearing Behavior in Public Spaces</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mohammed Guiga </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Abstract</h1>
<p>Advances in deep learning have brought the technology to a point of maturity where many pre-trained models exist for common tasks, such as object detection. As a result, smaller companies and industries who may have previously lacked the resources to invest in a machine learning department now have the ability to leverage this technology for their own benefit. To explore this further, this paper examines how the Department of Forest Resources, which may have traditionally been far away from software engineering and machine learning, could potentially use this technology to improve their operations. With the maturation of deep learning techniques, the department may now be able to leverage pre-existing models for tasks such as object detection and classification, which can have applications for forest conservation and management, as well as aiding public policy decision makers. The paper also explores the potential challenges and benefits of this approach. By leveraging pre-existing models, the Department of Forest Resources could gain a competitive edge while avoiding the significant investment of time and resources required to develop a machine learning department. The goal of this paper is to demonstrate the feasibility and potential benefits of leveraging deep learning models for practical applications in industries that previously lacked the resources to do so.</p>
</section>
<section id="introduction---todo" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Introduction - TODO</h1>
<ul>
<li>Background information and context of the research</li>
<li>Background information on the importance of mask-wearing during the COVID-19 pandemic</li>
<li>Research question or hypothesis</li>
<li>Significance of the study</li>
</ul>
</section>
<section id="literature-review---todo" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Literature Review - TODO</h1>
<ul>
<li>Summary of previous research on the topic</li>
<li>Gaps in the existing research that the current study addresses</li>
<li>Summary of previous research on measuring mask-wearing behavior</li>
<li>Discussion of the limitations of traditional methods for monitoring compliance</li>
<li>Overview of deep learning models and their potential for object detection and classification</li>
</ul>
</section>
<section id="methodology" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Methodology</h1>
<section id="research-design" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="research-design"><span class="header-section-number">4.1</span> Research Design</h2>
<p>The original research goal was to test the efficacy of 2D/3D image-based sensors to quantify COVID compliance behaviors, namely movement behavior and exposure risk, amongst public trail users. This involved quantifying physical distancing and mask wearing.</p>
</section>
<section id="data-collection" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">4.2</span> Data Collection</h2>
<p>A wireless 2D/3D image sensor prototype was deployed along typical public trail areas. Three locations were selected near amenities such as beaches, food/entertainment, and parks. MPRB permits were issued through summer 2021.</p>
<p>The location of the data collection sites are shown in <a href="#fig-lakes">Figure&nbsp;1</a>.</p>
<div id="fig-lakes" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-isles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_of_the_isles.png" class="img-fluid figure-img" data-ref-parent="fig-lakes"></p>
<p></p><figcaption class="figure-caption">(a) Lake of the Isles</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-maka" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/bde_maka_ska.png" class="img-fluid figure-img" data-ref-parent="fig-lakes"></p>
<p></p><figcaption class="figure-caption">(b) Bde Maka Ska</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-harriet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_harriet.png" class="img-fluid figure-img" data-ref-parent="fig-lakes"></p>
<p></p><figcaption class="figure-caption">(c) Lake Harriet</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Data Collection Locations</figcaption><p></p>
</figure>
</div>
<p>Examples of the data collection sites are shown in <a href="#fig-trails">Figure&nbsp;2</a>.</p>
<div id="fig-trails" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-isles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_of_the_isles_img.jpg" class="img-fluid figure-img" data-ref-parent="fig-trails"></p>
<p></p><figcaption class="figure-caption">(a) Lake of the Isles</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-maka" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/bde_maka_ska_img.jpg" class="img-fluid figure-img" data-ref-parent="fig-trails"></p>
<p></p><figcaption class="figure-caption">(b) Bde Maka Ska</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-harriet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_harriet_img.jpg" class="img-fluid figure-img" data-ref-parent="fig-trails"></p>
<p></p><figcaption class="figure-caption">(c) Lake Harriet</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Data Collection Locations Examples</figcaption><p></p>
</figure>
</div>
<p>Camera location for each of the data collection sites are shown in <a href="#fig-cameras">Figure&nbsp;3</a>.</p>
<div id="fig-cameras" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-isles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_of_the_isles_camera.png" class="img-fluid figure-img" data-ref-parent="fig-cameras"></p>
<p></p><figcaption class="figure-caption">(a) Lake of the Isles</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-maka" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/bde_maka_ska_camera.png" class="img-fluid figure-img" data-ref-parent="fig-cameras"></p>
<p></p><figcaption class="figure-caption">(b) Bde Maka Ska</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-harriet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_harriet_camera.png" class="img-fluid figure-img" data-ref-parent="fig-cameras"></p>
<p></p><figcaption class="figure-caption">(c) Lake Harriet</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Data Collection Camera Positions</figcaption><p></p>
</figure>
</div>
<p>This data was established as a feasible method of using a low-cost 2D/3D sensor system to capture detailed trail user movement behaviors, and the results were used to derive a metric to express ‘exposure risk’ using the detected trail user movement tracks.</p>
</section>
<section id="model-selection" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">4.3</span> Model Selection</h2>
<section id="object-detection-and-classification-methods" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="object-detection-and-classification-methods"><span class="header-section-number">4.3.1</span> Object Detection and Classification Methods</h3>
<p>Several models were evaluated for the tasks of face detection and mask classification. The first model tested was developed by a previous student who worked on this project. This model was trained on a new dataset consisting of real-world images collected from image search sites. The goal of this model was to predict mask-wearing behaviour. This model was not fine-tuned on the images collected from the trails, so it essentially acted as a pure off-the-shelf model. This model showed promising results on training and validation, and in this project was tested on a new real-world trail image dataset. The performance of this model on the real-world dataset is discussed in the results (see <a href="#sec-results">Section&nbsp;5</a>).</p>
<p>Additional off-the-shelf models were tested in the interest of evaluating the feasibility of these off-the-shelf models for real-world applications and industries that may not have access to machine learning expertice.</p>
</section>
<section id="metric-evaluation" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="metric-evaluation"><span class="header-section-number">4.3.2</span> Metric Evaluation</h3>
<p>Standard metrics were used to evaluate the models introduced above. The metrics chosen were sensitivity, specificity, precision, and accuracy. Sensitvity and specificity are not commonly found when it comes to evaluating deep learning models, but are common to the field of biostatistics, which is relevant to this project. Precision and recall were chosen since they are very common metrics used across the fields of computer science and machine learning. All four metrics were assessed across models and sub-groups, but the focal point will be sensitivity and specificity.</p>
<p>Sensitivity is defined as the probability of having a true positive given that you tested positive: <span class="math inline">\(P(T=1 | D=1)\)</span>, where T is the true result, and D is the test result.</p>
<p>Specificity is defined as the probability of having a true negative given that you tested negative: <span class="math inline">\(P(T=0 | D=0)\)</span>, where T is the true result, and D is the test result.</p>
</section>
</section>
</section>
<section id="sec-results" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results</h1>
<ul>
<li>Presentation of the findings, including accuracy rates and potential applications</li>
<li>Graphs and tables</li>
<li>Discussion of the challenges and limitations of the approach, such as privacy concerns and data quality</li>
</ul>
<section id="helena-nn" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="helena-nn"><span class="header-section-number">5.1</span> Helena NN</h2>
<p>The model provided by the research group was evaluated against the metrics defined above. The results can be seen in <a href="#tbl-metrics-full-dataset">Table&nbsp;1</a>. Plots can be seen in <a href="#fig-all-groups">Figure&nbsp;4 (a)</a>.</p>
<div id="fig-metrics_full_dataset" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-metrics-full-dataset" class="quarto-layout-cell anchored" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>Table&nbsp;1: Metrics - full dataset</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Lower bound</th>
<th style="text-align: center;">Upper bound</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sensitivity</td>
<td style="text-align: center;">0.7987</td>
<td style="text-align: center;">0.7614</td>
<td style="text-align: center;">0.8403</td>
</tr>
<tr class="even">
<td style="text-align: center;">Specificity</td>
<td style="text-align: center;">0.4302</td>
<td style="text-align: center;">0.3916</td>
<td style="text-align: center;">0.4675</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">0.5417</td>
<td style="text-align: center;">0.5094</td>
<td style="text-align: center;">0.5856</td>
</tr>
<tr class="even">
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.5988</td>
<td style="text-align: center;">0.5654</td>
<td style="text-align: center;">0.6233</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-all-groups" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/all_groups_metrics.png" class="img-fluid figure-img" data-ref-parent="fig-metrics_full_dataset"></p>
<p></p><figcaption class="figure-caption">(a) Metrics with Confidence Intervals</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Deep Neural Network Metrics</figcaption><p></p>
</figure>
</div>
<section id="sub-group-metric-evaluations" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="sub-group-metric-evaluations"><span class="header-section-number">5.1.1</span> Sub-group Metric Evaluations</h3>
<section id="image-examples" class="level4" data-number="5.1.1.1">
<h4 data-number="5.1.1.1" class="anchored" data-anchor-id="image-examples"><span class="header-section-number">5.1.1.1</span> Image Examples</h4>
<p>Quadrant 1 image examples can be seen in <a href="#fig-q1">Figure&nbsp;5</a></p>
<div id="fig-q1" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q1/q1_1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q1/q1_2.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Quadrant 1 Examples</figcaption><p></p>
</figure>
</div>
<p>Quadrant 3 image examples can be seen in <a href="#fig-q3">Figure&nbsp;6</a></p>
<div id="fig-q3" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q3/q3_1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q3/q3_2.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: Quadrant 3 Examples</figcaption><p></p>
</figure>
</div>
</section>
<section id="sub-group-metrics" class="level4" data-number="5.1.1.2">
<h4 data-number="5.1.1.2" class="anchored" data-anchor-id="sub-group-metrics"><span class="header-section-number">5.1.1.2</span> Sub-group Metrics</h4>
<p>See <a href="#fig-quadrants">Figure&nbsp;7</a> for metrics calculated across frame quadrants. The first and third quadrants represent the upper-left and upper-right quadrants of each frame, respectively. Quadrants 2 and 4 represent the bottom two quadrants of each frame, and in most cases are images of the ground. In this analysis, all faces were found in quadrants 1 and 3. Both quadrant 1 and quadrant 3 have poor specificity, with quadrant 3 having very poor specificity compared to the entire dataset. Both of these quadrants have high sensitivity also, which indicates overprediction independant of quadrant.</p>
<div id="fig-quadrants" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/quadrants_metrics.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: Metrics Across Quadrants</figcaption><p></p>
</figure>
</div>
<p>See <a href="#fig-dates">Figure&nbsp;8</a> for metrics calculated across dates. In terms of specificity, all dates have poor specificity, and high sensitivity, indicating overprediction independant of date.</p>
<div id="fig-dates" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/dates_metrics.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8: Metrics Across Dates</figcaption><p></p>
</figure>
</div>
<p>See <a href="#fig-quadrants-dates">Figure&nbsp;9</a> for metrics across a combination of quadrants and dates. The hypothesis is that since both the quadrants and dates yielded similar results, the combination will yield overpredicion as well.</p>
<div id="fig-quadrants-dates" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/quadrants_dates_metrics.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9: Metrics Across Quadrants and Dates</figcaption><p></p>
</figure>
</div>
<p>As can be seen in <a href="#fig-quadrants-dates">Figure&nbsp;9</a>, each sub-group has high sensitivity and low specificity, which aligns with the overprediction hypothesis.</p>
<p>The lower bound and upper bounds of the metrics were calculated by bootstrapping a 95% confidence interval. Bootstrap methods, in general, consist of estimating a characteristic of the unknown population by simulating the characteristic when the true population is replaced by an esitmated one <span class="citation" data-cites="diciccio1988review">(see <a href="#ref-diciccio1988review" role="doc-biblioref">Diciccio and Romano 1988</a>)</span>. The 95th percentile method was used to calculate a set of approximate confidence limits for the metrics defined above.</p>
<p>The metric results for the full dataset provided some insights on the limitations of the trained model. A noteworthy limitation is the specificity equal to 0.4 and the sensitificy equal to 0.8. The relatively high sensitivity paired with a low specificity shows that this model overpredics mask wearing. Additional sub-groups were analyzed to see if the model performed better in different scenarios.</p>
</section>
</section>
</section>
<section id="deepface" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="deepface"><span class="header-section-number">5.2</span> DeepFace</h2>
</section>
<section id="retinaface" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="retinaface"><span class="header-section-number">5.3</span> RetinaFace</h2>
</section>
<section id="off-the-shelf-mask-inference-model" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="off-the-shelf-mask-inference-model"><span class="header-section-number">5.4</span> Off-the-shelf Mask Inference Model</h2>
</section>
<section id="original-nn-vs-off-the-shelf-mask-inference-model" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="original-nn-vs-off-the-shelf-mask-inference-model"><span class="header-section-number">5.5</span> Original NN vs Off-the-shelf Mask Inference Model</h2>
</section>
</section>
<section id="conclusion---todo" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion - TODO</h1>
<ul>
<li>Summary of the main findings and their implications</li>
<li>Limitations of the study and suggestions for future research</li>
<li>Suggestions for future research and improvements to the approach</li>
<li>Discussion of the potential impact of deep learning models on public health policy and compliance monitoring</li>
</ul>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-diciccio1988review" class="csl-entry" role="listitem">
Diciccio, Thomas J, and Joseph P Romano. 1988. <span>“A Review of Bootstrap Confidence Intervals.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 50 (3): 338–54.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>