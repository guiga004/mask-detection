<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.302">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mohammed Guiga">

<title>Deep Learning Models for Measuring Mask-Wearing Behavior in Public Spaces</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="final_report_files/libs/clipboard/clipboard.min.js"></script>
<script src="final_report_files/libs/quarto-html/quarto.js"></script>
<script src="final_report_files/libs/quarto-html/popper.min.js"></script>
<script src="final_report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="final_report_files/libs/quarto-html/anchor.min.js"></script>
<link href="final_report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="final_report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="final_report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="final_report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="final_report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deep Learning Models for Measuring Mask-Wearing Behavior in Public Spaces</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mohammed Guiga </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Abstract</h1>
<p>Advances in deep learning have brought the technology to a point of maturity where many pre-trained models exist for common tasks, such as object detection. As a result, smaller companies and industries who may have previously lacked the resources to invest in a machine learning department now have the ability to leverage this technology for their own benefit. To explore this further, this paper examines how the Department of Forest Resources, which may have traditionally been far away from software engineering and machine learning, could potentially use this technology to improve their operations. With the maturation of deep learning techniques, the department may now be able to leverage pre-existing models for tasks such as object detection and classification, which can have applications for forest conservation and management, as well as aiding public policy decision makers. The paper also explores the potential challenges and benefits of this approach. By leveraging pre-existing models, the Department of Forest Resources could gain a competitive edge while avoiding the significant investment of time and resources required to develop a machine learning department. The goal of this paper is to demonstrate the feasibility and potential benefits of leveraging deep learning models for practical applications in industries that previously lacked the resources to do so.</p>
</section>
<section id="introduction---todo" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Introduction - TODO</h1>
<ul>
<li>Background information and context of the research</li>
<li>Background information on the importance of mask-wearing during the COVID-19 pandemic</li>
<li>Research question or hypothesis</li>
<li>Significance of the study</li>
</ul>
</section>
<section id="literature-review---todo" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Literature Review - TODO</h1>
<ul>
<li>Summary of previous research on the topic</li>
<li>Gaps in the existing research that the current study addresses</li>
<li>Summary of previous research on measuring mask-wearing behavior</li>
<li>Discussion of the limitations of traditional methods for monitoring compliance</li>
<li>Overview of deep learning models and their potential for object detection and classification</li>
</ul>
</section>
<section id="methods" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Methods</h1>
<section id="research-design" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="research-design"><span class="header-section-number">4.1</span> Research Design</h2>
<p>The original research goal was to test the efficacy of 2D/3D image-based sensors to quantify COVID compliance behaviors, namely movement behavior and exposure risk, amongst public trail users. This involved quantifying physical distancing and mask wearing.</p>
</section>
<section id="data-collection" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">4.2</span> Data Collection</h2>
<p>A wireless 2D/3D image sensor prototype was deployed along typical public trail areas. Three locations were selected near amenities such as beaches, food/entertainment, and parks. MPRB permits were issued through summer 2021.</p>
<p>The location of the data collection sites are shown in <a href="#fig-lakes">Figure&nbsp;1</a>.</p>
<div id="fig-lakes" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-isles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_of_the_isles.png" class="img-fluid figure-img" data-ref-parent="fig-lakes"></p>
<p></p><figcaption class="figure-caption">(a) Lake of the Isles</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-maka" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/bde_maka_ska.png" class="img-fluid figure-img" data-ref-parent="fig-lakes"></p>
<p></p><figcaption class="figure-caption">(b) Bde Maka Ska</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-harriet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_harriet.png" class="img-fluid figure-img" data-ref-parent="fig-lakes"></p>
<p></p><figcaption class="figure-caption">(c) Lake Harriet</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Data Collection Locations</figcaption><p></p>
</figure>
</div>
<p>Examples of the data collection sites are shown in <a href="#fig-trails">Figure&nbsp;2</a>.</p>
<div id="fig-trails" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-isles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_of_the_isles_img.jpg" class="img-fluid figure-img" data-ref-parent="fig-trails"></p>
<p></p><figcaption class="figure-caption">(a) Lake of the Isles</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-maka" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/bde_maka_ska_img.jpg" class="img-fluid figure-img" data-ref-parent="fig-trails"></p>
<p></p><figcaption class="figure-caption">(b) Bde Maka Ska</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-harriet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_harriet_img.jpg" class="img-fluid figure-img" data-ref-parent="fig-trails"></p>
<p></p><figcaption class="figure-caption">(c) Lake Harriet</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Data Collection Locations Examples</figcaption><p></p>
</figure>
</div>
<p>Camera location for each of the data collection sites are shown in <a href="#fig-cameras">Figure&nbsp;3</a>.</p>
<div id="fig-cameras" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-isles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_of_the_isles_camera.png" class="img-fluid figure-img" data-ref-parent="fig-cameras"></p>
<p></p><figcaption class="figure-caption">(a) Lake of the Isles</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-maka" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/bde_maka_ska_camera.png" class="img-fluid figure-img" data-ref-parent="fig-cameras"></p>
<p></p><figcaption class="figure-caption">(b) Bde Maka Ska</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-harriet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/camera/lake_harriet_camera.png" class="img-fluid figure-img" data-ref-parent="fig-cameras"></p>
<p></p><figcaption class="figure-caption">(c) Lake Harriet</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Data Collection Camera Positions</figcaption><p></p>
</figure>
</div>
<p>This data was established as a feasible method of using a low-cost 2D/3D sensor system to capture detailed trail user movement behaviors, and the results were used to derive a metric to express ‘exposure risk’ using the detected trail user movement tracks.</p>
</section>
<section id="model-selection" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">4.3</span> Model Selection</h2>
<section id="object-detection-and-classification-methods" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="object-detection-and-classification-methods"><span class="header-section-number">4.3.1</span> Object Detection and Classification Methods</h3>
<p>Several models were evaluated for the tasks of face detection and mask classification. The first model tested was developed by a previous student who worked on this project. This model, called Xception, was trained on a new dataset consisting of real-world images collected from image search sites. The goal of this model was to predict mask-wearing behaviour. However, this model was not fine-tuned on the images collected from the trails, so it essentially acted as a pure off-the-shelf model. This model showed promising results on training and validation, and in this project was tested on a new real-world trail image dataset. The performance of this model on the real-world dataset is discussed in the results (see <a href="#sec-results">Section&nbsp;5</a>).</p>
<p>An additional off-the-shelf model was tested in the interest of evaluating the feasibility of for real-world applications and industries that may not have access to machine learning expertice, such as parks and forestry.</p>
<section id="previous-mask-classifier-architecture" class="level4" data-number="4.3.1.1">
<h4 data-number="4.3.1.1" class="anchored" data-anchor-id="previous-mask-classifier-architecture"><span class="header-section-number">4.3.1.1</span> Previous Mask-Classifier Architecture</h4>
<p>The mask classifier designed by a previous student worked as follows:</p>
<ol type="1">
<li>Apply RetinaNet face detection model to generate face crops</li>
<li>Feed cropped face to Xception model for mask classification</li>
<li>Final output is the face detected by RetinaFace along with the mask prediction</li>
</ol>
<p>This architecture uses a pre-trained face detection model called RetinaNet, which was chosen due to it’s supperior performance against other algorithms such as MT-CNN, Haar Cascade and HOG. RetinaNet achieves higher recall and precision than the aformentioned algorithms.</p>
<p>The Xception model is used in this architecture for the mask classification portion. Transfer learning was employed to use the exising model weights of the Xception model, and additional learning was completed in an iterative manner to finetune against the newly gathered training set.</p>
<p>This classifier was evaluated based on F1-score and the Loss. It was demonstrated to achieve an F1-score of 0.99, and a loss of 0.0182.</p>
</section>
<section id="new-mask-classifier-architecture" class="level4" data-number="4.3.1.2">
<h4 data-number="4.3.1.2" class="anchored" data-anchor-id="new-mask-classifier-architecture"><span class="header-section-number">4.3.1.2</span> New Mask-Classifier Architecture</h4>
<p>A new architecture was tested on the trails image set. This architecture was chosen from several pre-trained options, and was evaluated against the dataset and the previous Xception model. This architecture is a pure off-the-shelf solution, and was shown to have promise when confronted with unseen real-world data. The model tested is a FaceMaskDetection model developed by a group called AIZOOtech. This model was trained on 7971 images composed of WIDER FACE and MAFA. WIDER FACE is a face detection benchmark dataset that contains images from the publicaly available WIDER dataset. This dataset containes images with a high degree of variability in scale, pose, and occlusion. This variablity can help deter the model from overfitting during training. AIZOO provides API access to their model via PyTorch, TensorFlow, Keras, MXNet, Caffee, Paddle, and OpenCV dnn. The PyTorch framework was used for this analysis.</p>
<p>The model structure can be seen in <a href="#fig-aizoo-model">Figure&nbsp;4</a>.</p>
<div id="fig-aizoo-model" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/models/AIZOOModel.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: AIZOO Model Structure</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="metric-evaluation" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="metric-evaluation"><span class="header-section-number">4.3.2</span> Metric Evaluation</h3>
<p>Standard metrics were used to evaluate the models introduced above. The metrics chosen were sensitivity, specificity, precision, and accuracy. Sensitvity and specificity are not commonly found when it comes to evaluating deep learning models, but are common to the field of biostatistics, which is relevant to this project. Precision and recall were chosen since they are very common metrics used across the fields of computer science and machine learning. All four metrics were assessed across models and sub-groups, but the focal point was sensitivity and specificity.</p>
<p>Sensitivity is defined as the probability of having a true positive given that you tested positive: <span class="math inline">\(P(T=1 | D=1)\)</span>, where T is the true result, and D is the test result.</p>
<p>Specificity is defined as the probability of having a true negative given that you tested negative: <span class="math inline">\(P(T=0 | D=0)\)</span>, where T is the true result, and D is the test result.</p>
</section>
<section id="additional-models-tested" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="additional-models-tested"><span class="header-section-number">4.3.3</span> Additional Models Tested</h3>
<section id="deepface" class="level4" data-number="4.3.3.1">
<h4 data-number="4.3.3.1" class="anchored" data-anchor-id="deepface"><span class="header-section-number">4.3.3.1</span> DeepFace</h4>
<p>DeepFace is an off-the-shelf facial recognition framework for python. The library includes access to state-of-the-art models such as VGG-Face, Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace. The performance of DeepFace is discussed in <a href="#sec-deepface-results">Section&nbsp;5.2</a>.</p>
</section>
<section id="retinaface" class="level4" data-number="4.3.3.2">
<h4 data-number="4.3.3.2" class="anchored" data-anchor-id="retinaface"><span class="header-section-number">4.3.3.2</span> RetinaFace</h4>
<p>The performance of RetinaFace is discussed in <a href="#sec-retinaface-results">Section&nbsp;5.3</a>.</p>
</section>
</section>
</section>
</section>
<section id="sec-results" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results</h1>
<ul>
<li>Presentation of the findings, including accuracy rates and potential applications</li>
<li>Graphs and tables</li>
<li>Discussion of the challenges and limitations of the approach, such as privacy concerns and data quality</li>
</ul>
<section id="xception-model" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="xception-model"><span class="header-section-number">5.1</span> Xception Model</h2>
<p>The model provided by the research group was evaluated against the metrics defined above. The results can be seen in <a href="#tbl-metrics-full-dataset">Table&nbsp;1</a> and plots can be found in <a href="#fig-all-groups">Figure&nbsp;5 (a)</a>.</p>
<div id="fig-metrics_full_dataset" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-metrics-full-dataset" class="quarto-layout-cell anchored" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>Table&nbsp;1: Metrics - full dataset</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Lower bound</th>
<th style="text-align: center;">Upper bound</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sensitivity</td>
<td style="text-align: center;">0.7987</td>
<td style="text-align: center;">0.7614</td>
<td style="text-align: center;">0.8403</td>
</tr>
<tr class="even">
<td style="text-align: center;">Specificity</td>
<td style="text-align: center;">0.4302</td>
<td style="text-align: center;">0.3916</td>
<td style="text-align: center;">0.4675</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">0.5417</td>
<td style="text-align: center;">0.5094</td>
<td style="text-align: center;">0.5856</td>
</tr>
<tr class="even">
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.5988</td>
<td style="text-align: center;">0.5654</td>
<td style="text-align: center;">0.6233</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-all-groups" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/all_groups_metrics.png" class="img-fluid figure-img" data-ref-parent="fig-metrics_full_dataset"></p>
<p></p><figcaption class="figure-caption">(a) Metrics with Confidence Intervals</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Deep Neural Network Metrics</figcaption><p></p>
</figure>
</div>
<p>The lower bound and upper bounds of the metrics were calculated by bootstrapping a 95% confidence interval. Bootstrap methods, in general, consist of estimating a characteristic of the unknown population by simulating the characteristic when the true population is replaced by an esitmated one <span class="citation" data-cites="diciccio1988review">(see <a href="#ref-diciccio1988review" role="doc-biblioref">Diciccio and Romano 1988</a>)</span>. The 95th percentile method was used to calculate a set of approximate confidence limits for the metrics defined above.</p>
<p>The metric results for the full dataset provided some insights on the limitations of the trained model. A noteworthy limitation is the specificity equal to 0.4 and the sensitificy equal to 0.8. The relatively high sensitivity paired with a low specificity shows that this model overpredics mask wearing. Additional sub-groups were analyzed to see if the model performed better in different scenarios.</p>
<section id="sub-group-metric-evaluations" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="sub-group-metric-evaluations"><span class="header-section-number">5.1.1</span> Sub-group Metric Evaluations</h3>
<p>See <a href="#fig-quadrants">Figure&nbsp;6</a> for metrics calculated across frame quadrants. The first and third quadrants represent the upper-left and upper-right quadrants of each frame, respectively. Quadrants 2 and 4 represent the bottom two quadrants of each frame, and in most cases are images of the ground. In this analysis, all faces were found in quadrants 1 and 3. Both quadrant 1 and quadrant 3 have poor specificity, with quadrant 3 having very poor specificity compared to the entire dataset. Both of these quadrants have high sensitivity also, which indicates overprediction independant of quadrant.</p>
<div id="fig-quadrants" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/quadrants_metrics.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: Metrics Across Quadrants</figcaption><p></p>
</figure>
</div>
<p>See <a href="#fig-dates">Figure&nbsp;7</a> for metrics calculated across dates. In terms of specificity, all dates have poor specificity, and high sensitivity, indicating overprediction independant of date.</p>
<div id="fig-dates" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/dates_metrics.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: Metrics Across Dates</figcaption><p></p>
</figure>
</div>
<p>See <a href="#fig-quadrants-dates">Figure&nbsp;8</a> for metrics across a combination of quadrants and dates. The hypothesis is that since both the quadrants and dates yielded similar results, the combination will yield overpredicion as well.</p>
<div id="fig-quadrants-dates" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/metrics/quadrants_dates_metrics.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8: Metrics Across Quadrants and Dates</figcaption><p></p>
</figure>
</div>
<p>As can be seen in <a href="#fig-quadrants-dates">Figure&nbsp;8</a>, each sub-group has high sensitivity and low specificity, which aligns with the overprediction hypothesis.</p>
</section>
<section id="image-examples" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="image-examples"><span class="header-section-number">5.1.2</span> Image Examples</h3>
<p>Quadrant 1 image examples can be seen in <a href="#fig-q1">Figure&nbsp;9</a></p>
<div id="fig-q1" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q1/q1_1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q1/q1_2.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;9: Quadrant 1 Examples</figcaption><p></p>
</figure>
</div>
<p>Quadrant 3 image examples can be seen in <a href="#fig-q3">Figure&nbsp;10</a></p>
<div id="fig-q3" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q3/q3_1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="final_report_images/examples/q3/q3_2.png" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;10: Quadrant 3 Examples</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="sec-deepface-results" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-deepface-results"><span class="header-section-number">5.2</span> DeepFace</h2>
<p>The results of the face detectors are shown in <a href="#fig-deepface">Figure&nbsp;11</a>. The Xception architecture was able to find and draw a bounding box on one of two visible faces in the image. The DeepFace model, using the retinaface backend, succesfully found both visible faces.</p>
<div id="fig-deepface" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-deepface-og" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/deepface/og.png" class="img-fluid figure-img" data-ref-parent="fig-deepface"></p>
<p></p><figcaption class="figure-caption">(a) Original Image</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-deepface-xception" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/deepface/xception.png" class="img-fluid figure-img" data-ref-parent="fig-deepface"></p>
<p></p><figcaption class="figure-caption">(b) Xception</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-deepface-deepface" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/deepface/deepface.png" class="img-fluid figure-img" data-ref-parent="fig-deepface"></p>
<p></p><figcaption class="figure-caption">(c) DeepFace</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;11: Facial Detection</figcaption><p></p>
</figure>
</div>
</section>
<section id="sec-retinaface-results" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sec-retinaface-results"><span class="header-section-number">5.3</span> RetinaFace</h2>
<p>The retinaface backend proved successful when testing the DeepFace framework. Retinaface also provides the capability to extract facial landmarks, such as the eyes, nose, and mouth corners. One hypothesis was to use the facial landmarks, or lack-there-of, to infer whether or not a mask is being worn. However, as can be see in <a href="#fig-retina-mask2">Figure&nbsp;12 (d)</a>, the RetinaFace algorithm attempted to find facial landmark points regardless of mask-wearing.</p>
<div id="fig-retinaface" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-retina1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/retinaface/retinaface_1.png" class="img-fluid figure-img" data-ref-parent="fig-retinaface"></p>
<p></p><figcaption class="figure-caption">(a) Facial Landmark Extraction</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-retina2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/retinaface/retinaface_2.png" class="img-fluid figure-img" data-ref-parent="fig-retinaface"></p>
<p></p><figcaption class="figure-caption">(b) Facial Landmark Extraction</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-retina-mask1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/retinaface/retinaface_mask_1.png" class="img-fluid figure-img" data-ref-parent="fig-retinaface"></p>
<p></p><figcaption class="figure-caption">(c) Original Image</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-retina-mask2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/retinaface/retinaface_mask_2.png" class="img-fluid figure-img" data-ref-parent="fig-retinaface"></p>
<p></p><figcaption class="figure-caption">(d) Facial Landmark Extraction</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;12: Facial Landmark Detection</figcaption><p></p>
</figure>
</div>
</section>
<section id="aizoo-model" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="aizoo-model"><span class="header-section-number">5.4</span> AIZOO Model</h2>
<div id="fig-aizoo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/aizoo/aizoo.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;13: AIZOO Inference</figcaption><p></p>
</figure>
</div>
<div id="fig-aizoo-metrics" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-aizoo-metrics" class="quarto-layout-cell anchored" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>Table&nbsp;2: Metrics - AIZOO</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Lower bound</th>
<th style="text-align: center;">Upper bound</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sensitivity</td>
<td style="text-align: center;">0.5199</td>
<td style="text-align: center;">0.4778</td>
<td style="text-align: center;">0.5740</td>
</tr>
<tr class="even">
<td style="text-align: center;">Specificity</td>
<td style="text-align: center;">0.9804</td>
<td style="text-align: center;">0.9642</td>
<td style="text-align: center;">0.9932</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">0.9676</td>
<td style="text-align: center;">0.9326</td>
<td style="text-align: center;">0.9864</td>
</tr>
<tr class="even">
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.7386</td>
<td style="text-align: center;">0.7118</td>
<td style="text-align: center;">0.7698</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-aizoo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/aizoo/aizoo_metrics.png" class="img-fluid figure-img" data-ref-parent="fig-aizoo-metrics"></p>
<p></p><figcaption class="figure-caption">(a) AIZOO Inference</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;14: AIZOO Metrics</figcaption><p></p>
</figure>
</div>
</section>
<section id="xception-vs-aizoo" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="xception-vs-aizoo"><span class="header-section-number">5.5</span> Xception vs AIZOO</h2>
<div id="fig-aizoo-xception" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="final_report_images/aizoo/aizoo_vs_xception.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;15: AIZOO Inference</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="conclusion---todo" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion - TODO</h1>
<ul>
<li>Summary of the main findings and their implications</li>
<li>Limitations of the study and suggestions for future research</li>
<li>Suggestions for future research and improvements to the approach</li>
<li>Discussion of the potential impact of deep learning models on public health policy and compliance monitoring</li>
</ul>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-diciccio1988review" class="csl-entry" role="listitem">
Diciccio, Thomas J, and Joseph P Romano. 1988. <span>“A Review of Bootstrap Confidence Intervals.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 50 (3): 338–54.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>