{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b52af9f3-56f6-42dc-807e-d2c2200f3753",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Deep Learning Models for Measuring Mask-Wearing Behavior in Public Spaces\"\n",
    "author: \"Mohammed Guiga\"\n",
    "toc: false\n",
    "number-sections: true\n",
    "bibliography: final_report.bib\n",
    "notes-after-punctuation: true\n",
    "link-citations: true\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-line-numbers: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd97f4-6c4e-4b65-9fc9-d4cc1cd21945",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9808376-0374-4936-b2e8-17de0d43c4d8",
   "metadata": {},
   "source": [
    "Advances in deep learning have brought the technology to a point of maturity where many pre-trained models exist for common tasks, such as object detection. As a result, smaller companies and industries who may have previously lacked the resources to invest in a machine learning department now have the ability to leverage this technology for their own benefit. To explore this further, this paper examines how the Department of Forest Resources, which may have traditionally been far away from software engineering and machine learning, could potentially use this technology to improve their operations. With the maturation of deep learning techniques, the department may now be able to leverage pre-existing models for tasks such as object detection and classification, which can have applications for forest conservation and management, as well as aiding public policy decision makers. The paper also explores the potential challenges and benefits of this approach. By leveraging pre-existing models, the Department of Forest Resources could gain a competitive edge while avoiding the significant investment of time and resources required to develop a machine learning department. The goal of this paper is to demonstrate the feasibility and potential benefits of leveraging deep learning models for practical applications in industries that previously lacked the resources to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea809047-9b03-4cf4-8e42-e01c89afa146",
   "metadata": {},
   "source": [
    "# Introduction - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d2e7b-3862-409c-a69b-fe52762cde74",
   "metadata": {},
   "source": [
    "* Background information and context of the research\n",
    "* Background information on the importance of mask-wearing during the COVID-19 pandemic\n",
    "* Research question or hypothesis\n",
    "* Significance of the study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee3e63-b66b-4ad3-9915-f8a73c51a6ce",
   "metadata": {},
   "source": [
    "# Literature Review - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcab1d-14fa-4fb2-9a6e-37f990f3effa",
   "metadata": {},
   "source": [
    "* Summary of previous research on the topic\n",
    "* Gaps in the existing research that the current study addresses\n",
    "* Summary of previous research on measuring mask-wearing behavior\n",
    "* Discussion of the limitations of traditional methods for monitoring compliance\n",
    "* Overview of deep learning models and their potential for object detection and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b3ccd-c2f6-4c5b-a8c0-ae5c26f9b298",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2e9b-7ac8-4e79-b7fb-69aff036327d",
   "metadata": {},
   "source": [
    "## Research Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18234aa4-1cb6-4866-a2d4-7ef980b11d16",
   "metadata": {},
   "source": [
    "The original research goal was to test the efficacy of 2D/3D image-based sensors to quantify COVID compliance behaviors, namely movement behavior and exposure risk, amongst public trail users. This involved quantifying physical distancing and mask wearing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cbed8-593e-4614-89f8-0af2be3992c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cec0b7-db07-417b-a0c9-b3ee261e315e",
   "metadata": {},
   "source": [
    "A wireless 2D/3D image sensor prototype was deployed along typical public trail areas. Three locations were selected near amenities such as beaches, food/entertainment, and parks. MPRB permits were issued through summer 2021."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed42ce0-2b0d-4e7b-9d08-4f3d2bf8d464",
   "metadata": {},
   "source": [
    "The location of the data collection sites are shown in @fig-lakes.\n",
    "\n",
    "::: {#fig-lakes layout-ncol=3}\n",
    "\n",
    "![Lake of the Isles](final_report_images/camera/lake_of_the_isles.png){#fig-isles}\n",
    "\n",
    "![Bde Maka Ska](final_report_images/camera/bde_maka_ska.png){#fig-maka}\n",
    "\n",
    "![Lake Harriet](final_report_images/camera/lake_harriet.png){#fig-harriet}\n",
    "\n",
    "Data Collection Locations\n",
    ":::"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d23664cd-b855-4e9a-9083-88c31933f50c",
   "metadata": {},
   "source": [
    "Examples of the data collection sites are shown in @fig-trails.\n",
    "\n",
    "::: {#fig-trails layout-ncol=3}\n",
    "\n",
    "![Lake of the Isles](final_report_images/camera/lake_of_the_isles_img.jpg){#fig-isles}\n",
    "\n",
    "![Bde Maka Ska](final_report_images/camera/bde_maka_ska_img.jpg){#fig-maka}\n",
    "\n",
    "![Lake Harriet](final_report_images/camera/lake_harriet_img.jpg){#fig-harriet}\n",
    "\n",
    "Data Collection Locations Examples\n",
    ":::"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e43b0dc5-fc1f-43f5-9ea5-86900c0fc9c5",
   "metadata": {},
   "source": [
    "Camera location for each of the data collection sites are shown in @fig-cameras.\n",
    "\n",
    "::: {#fig-cameras layout-ncol=3}\n",
    "\n",
    "![Lake of the Isles](final_report_images/camera/lake_of_the_isles_camera.png){#fig-isles}\n",
    "\n",
    "![Bde Maka Ska](final_report_images/camera/bde_maka_ska_camera.png){#fig-maka}\n",
    "\n",
    "![Lake Harriet](final_report_images/camera/lake_harriet_camera.png){#fig-harriet}\n",
    "\n",
    "Data Collection Camera Positions\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea3840-879b-481b-af0d-452bbbf2b657",
   "metadata": {},
   "source": [
    "This data was established as a feasible method of using a low-cost 2D/3D sensor system to capture detailed trail user movement behaviors, and the results were used to derive a metric to express 'exposure risk' using the detected trail user movement tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22906728-ee9d-4b21-a4a3-e60ab35b721f",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929ca05-af1a-4333-ab8d-2be09e9c6d86",
   "metadata": {},
   "source": [
    "### Object Detection and Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331d81e-05b5-4032-a8c5-19bd71cc97fd",
   "metadata": {},
   "source": [
    "Several models were evaluated for the tasks of face detection and mask classification. The first model tested was developed by a previous student who worked on this project. This model, called Xception, was trained on a new dataset consisting of real-world images collected from image search sites. The goal of this model was to predict mask-wearing behaviour. However, this model was not fine-tuned on the images collected from the trails, so it essentially acted as a pure off-the-shelf model. This model showed promising results on training and validation, and in this project was tested on a new real-world trail image dataset. The performance of this model on the real-world dataset is discussed in the results (see @sec-results).\n",
    "\n",
    "An additional off-the-shelf model was tested in the interest of evaluating the feasibility of for real-world applications and industries that may not have access to machine learning expertice, such as parks and forestry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e1638-0e3c-4120-afe9-f3f24caacd54",
   "metadata": {},
   "source": [
    "#### Previous Mask-Classifier Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b22da-4b86-433b-a5d4-0a0319232b8c",
   "metadata": {},
   "source": [
    "The mask classifier designed by a previous student worked as follows:\n",
    "\n",
    "1. Apply RetinaNet face detection model to generate face crops\n",
    "2. Feed cropped face to Xception model for mask classification\n",
    "3. Final output is the face detected by RetinaFace along with the mask prediction\n",
    "\n",
    "This architecture uses a pre-trained face detection model called RetinaNet, which was chosen due to it's supperior performance against other algorithms such as MT-CNN, Haar Cascade and HOG. RetinaNet achieves higher recall and precision than the aformentioned algorithms. \n",
    "\n",
    "The Xception model is used in this architecture for the mask classification portion. Transfer learning was employed to use the exising model weights of the Xception model, and additional learning was completed in an iterative manner to finetune against the newly gathered training set. \n",
    "\n",
    "This classifier was evaluated based on F1-score and the Loss. It was demonstrated to achieve an F1-score of 0.99, and a loss of 0.0182."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d06c0-1a8a-475a-bfcb-7eba90540fc9",
   "metadata": {},
   "source": [
    "#### New Mask-Classifier Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ff136-ab49-4187-85be-1c7232654d9b",
   "metadata": {},
   "source": [
    "A new architecture was tested on the trails image set. This architecture was chosen from several pre-trained options, and was evaluated against the dataset and the previous Xception model. This architecture is a pure off-the-shelf solution, and was shown to have promise when confronted with unseen real-world data. The model tested is a FaceMaskDetection model developed by a group called AIZOOtech. This model was trained on 7971 images composed of WIDER FACE and MAFA. WIDER FACE is a face detection benchmark dataset that contains images from the publicaly available WIDER dataset. This dataset containes images with a high degree of variability in scale, pose, and occlusion. This variablity can help deter the model from overfitting during training. AIZOO provides API access to their model via PyTorch, TensorFlow, Keras, MXNet, Caffee, Paddle, and OpenCV dnn. The PyTorch framework was used for this analysis.\n",
    "\n",
    "The model structure can be seen in @fig-aizoo-model.\n",
    "\n",
    "![AIZOO Model Structure](final_report_images/models/AIZOOModel.png){#fig-aizoo-model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56136d1-07ef-4dc6-83ce-0a71ab31ca56",
   "metadata": {},
   "source": [
    "### Metric Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac37eb-a764-45fe-9727-affe42e1b3f6",
   "metadata": {},
   "source": [
    "Standard metrics were used to evaluate the models introduced above. The metrics chosen were sensitivity, specificity, precision, and accuracy. Sensitvity and specificity are not commonly found when it comes to evaluating deep learning models, but are common to the field of biostatistics, which is relevant to this project. Precision and recall were chosen since they are very common metrics used across the fields of computer science and machine learning. All four metrics were assessed across models and sub-groups, but the focal point was sensitivity and specificity.\n",
    "\n",
    "Sensitivity is defined as the probability of having a true positive given that you tested positive: $P(T=1 | D=1)$, where T is the true result, and D is the test result.\n",
    "\n",
    "Specificity is defined as the probability of having a true negative given that you tested negative: $P(T=0 | D=0)$, where T is the true result, and D is the test result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990a717-e017-407e-aad9-4e7916362a2c",
   "metadata": {},
   "source": [
    "### Additional Models Tested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cc4da-147e-4ec0-abdc-fe5924909651",
   "metadata": {},
   "source": [
    "#### DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709fb1c-7b77-4e7f-b6eb-6055f52fdc0d",
   "metadata": {},
   "source": [
    "DeepFace is an off-the-shelf facial recognition framework for python. The library includes access to state-of-the-art models such as VGG-Face, Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace. The performance of DeepFace is discussed in @sec-deepface-results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3864820-66f1-443f-870c-ed1e7184fa6f",
   "metadata": {},
   "source": [
    "#### RetinaFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84087c4-37ef-4b72-8297-da451d2b2230",
   "metadata": {},
   "source": [
    "The performance of RetinaFace is discussed in @sec-retinaface-results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ebe8e-98db-4997-a92b-0b960c4bb9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results {#sec-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a44d7-17a1-47c4-86a3-f6d2aa10b56a",
   "metadata": {},
   "source": [
    "* Presentation of the findings, including accuracy rates and potential applications\n",
    "* Graphs and tables\n",
    "* Discussion of the challenges and limitations of the approach, such as privacy concerns and data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62988b9b-bb38-4e6b-a64e-36e199c5a8e9",
   "metadata": {},
   "source": [
    "## Xception Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9308f95-7afc-41f0-a673-f9647a323152",
   "metadata": {},
   "source": [
    "The model provided by the research group was evaluated against the metrics defined above. The results can be seen in @tbl-metrics-full-dataset and plots can be found in @fig-all-groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dd813-2bac-4b79-a735-bed9f736fc3c",
   "metadata": {},
   "source": [
    "::: {#fig-metrics_full_dataset layout-ncol=2}\n",
    "\n",
    "|    Metric   |  Score | Lower bound | Upper bound |\n",
    "|:-----------:|:------:|:-----------:|:-----------:|\n",
    "| Sensitivity | 0.7987 | 0.7614      | 0.8403      |\n",
    "| Specificity | 0.4302 | 0.3916      | 0.4675      |\n",
    "| Precision   | 0.5417 | 0.5094      | 0.5856      |\n",
    "| Accuracy    | 0.5988 | 0.5654      | 0.6233      |\n",
    "\n",
    ": Metrics - full dataset {#tbl-metrics-full-dataset}\n",
    "\n",
    "![Metrics with Confidence Intervals](final_report_images/metrics/all_groups_metrics.png){#fig-all-groups}\n",
    "\n",
    "Deep Neural Network Metrics\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f764d-315d-4ca4-a460-0ff332022797",
   "metadata": {},
   "source": [
    "The lower bound and upper bounds of the metrics were calculated by bootstrapping a 95% confidence interval. Bootstrap methods, in general, consist of estimating a characteristic of the unknown population by simulating the characteristic when the true population is replaced by an esitmated one [see @diciccio1988review]. The 95th percentile method was used to calculate a set of approximate confidence limits for the metrics defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312513b5-685a-4164-b8d5-deeb59fce65c",
   "metadata": {},
   "source": [
    "The metric results for the full dataset provided some insights on the limitations of the trained model. A noteworthy limitation is the specificity equal to 0.4 and the sensitificy equal to 0.8. The relatively high sensitivity paired with a low specificity shows that this model overpredics mask wearing. Additional sub-groups were analyzed to see if the model performed better in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8bfe-fa1b-47a2-9099-aec2431dc34f",
   "metadata": {},
   "source": [
    "### Sub-group Metric Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819366d-20bd-44fd-9e72-321501387930",
   "metadata": {},
   "source": [
    "See @fig-quadrants for metrics calculated across frame quadrants. The first and third quadrants represent the upper-left and upper-right quadrants of each frame, respectively. Quadrants 2 and 4 represent the bottom two quadrants of each frame, and in most cases are images of the ground. In this analysis, all faces were found in quadrants 1 and 3. Both quadrant 1 and quadrant 3 have poor specificity, with quadrant 3 having very poor specificity compared to the entire dataset. Both of these quadrants have high sensitivity also, which indicates overprediction independant of quadrant.\n",
    "\n",
    "![Metrics Across Quadrants](final_report_images/metrics/quadrants_metrics.png){#fig-quadrants width=80%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331ba86-0579-4fe1-90ca-6bba16c4344a",
   "metadata": {},
   "source": [
    "See @fig-dates for metrics calculated across dates. In terms of specificity, all dates have poor specificity, and high sensitivity, indicating overprediction independant of date.\n",
    "\n",
    "![Metrics Across Dates](final_report_images/metrics/dates_metrics.png){#fig-dates width=80%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb4357-c9d0-4c49-8a4b-b79172e4b735",
   "metadata": {},
   "source": [
    "See @fig-quadrants-dates for metrics across a combination of quadrants and dates. The hypothesis is that since both the quadrants and dates yielded similar results, the combination will yield overpredicion as well.\n",
    "\n",
    "![Metrics Across Quadrants and Dates](final_report_images/metrics/quadrants_dates_metrics.png){#fig-quadrants-dates width=80%}\n",
    "\n",
    "As can be seen in @fig-quadrants-dates, each sub-group has high sensitivity and low specificity, which aligns with the overprediction hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c7bb6-642c-4335-a168-c6886a6aca6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6e534-5748-4b25-b919-820fd5da2760",
   "metadata": {},
   "source": [
    "Quadrant 1 image examples can be seen in @fig-q1\n",
    "\n",
    "::: {#fig-q1 layout-ncol=2}\n",
    "\n",
    "![](final_report_images/examples/q1/q1_1.png)\n",
    "\n",
    "![](final_report_images/examples/q1/q1_2.png)\n",
    "\n",
    "Quadrant 1 Examples\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc6c05-6c9d-4de3-af5f-abbf5005d9ba",
   "metadata": {},
   "source": [
    "Quadrant 3 image examples can be seen in @fig-q3\n",
    "\n",
    "::: {#fig-q3 layout-ncol=2}\n",
    "\n",
    "![](final_report_images/examples/q3/q3_1.png)\n",
    "\n",
    "![](final_report_images/examples/q3/q3_2.png)\n",
    "\n",
    "Quadrant 3 Examples\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7f981-b8ae-4add-ba5b-23508ef60ae9",
   "metadata": {},
   "source": [
    "## DeepFace {#sec-deepface-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ddc71-3d1c-4ea9-8157-53489c5cf1e7",
   "metadata": {},
   "source": [
    "The results of the face detectors are shown in @fig-deepface. The Xception architecture was able to find and draw a bounding box on one of two visible faces in the image. The DeepFace model, using the retinaface backend, succesfully found both visible faces.\n",
    "\n",
    "::: {#fig-deepface layout-ncol=3}\n",
    "\n",
    "![Original Image](final_report_images/deepface/og.png){#fig-deepface-og}\n",
    "\n",
    "![Xception](final_report_images/deepface/xception.png){#fig-deepface-xception}\n",
    "\n",
    "![DeepFace](final_report_images/deepface/deepface.png){#fig-deepface-deepface}\n",
    "\n",
    "Facial Detection\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2dd3b0-635d-4b8d-8d07-f9c2b928c877",
   "metadata": {},
   "source": [
    "## RetinaFace {#sec-retinaface-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d4a54-5721-474a-b572-5ce068f84bcc",
   "metadata": {},
   "source": [
    "The retinaface backend proved successful when testing the DeepFace framework. Retinaface also provides the capability to extract facial landmarks, such as the eyes, nose, and mouth corners. One hypothesis was to use the facial landmarks, or lack-there-of, to infer whether or not a mask is being worn. However, as can be see in @fig-retina-mask2, the RetinaFace algorithm attempted to find facial landmark points regardless of mask-wearing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b196d24-aa4a-48fe-a4ad-d6cdb9938c57",
   "metadata": {},
   "source": [
    "::: {#fig-retinaface layout-ncol=2}\n",
    "\n",
    "![Facial Landmark Extraction](final_report_images/retinaface/retinaface_1.png){#fig-retina1}\n",
    "\n",
    "![Facial Landmark Extraction](final_report_images/retinaface/retinaface_2.png){#fig-retina2}\n",
    "\n",
    "![Original Image](final_report_images/retinaface/retinaface_mask_1.png){#fig-retina-mask1}\n",
    "\n",
    "![Facial Landmark Extraction](final_report_images/retinaface/retinaface_mask_2.png){#fig-retina-mask2}\n",
    "\n",
    "Facial Landmark Detection\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e4688-6a14-4693-8dd5-5fc42c4525d0",
   "metadata": {},
   "source": [
    "## AIZOO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d28734-1a87-49c1-9c1d-717510bfe076",
   "metadata": {},
   "source": [
    "![AIZOO Inference](final_report_images/aizoo/aizoo.png){#fig-aizoo width=75%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fb48a-8ea9-4ef5-b471-1d6af6f14217",
   "metadata": {},
   "source": [
    "::: {#fig-aizoo-metrics layout-ncol=2}\n",
    "\n",
    "|    Metric   |  Score | Lower bound | Upper bound |\n",
    "|:-----------:|:------:|:-----------:|:-----------:|\n",
    "| Sensitivity | 0.5199 | 0.4778      | 0.5740      |\n",
    "| Specificity | 0.9804 | 0.9642      | 0.9932      |\n",
    "| Precision   | 0.9676 | 0.9326      | 0.9864      |\n",
    "| Accuracy    | 0.7386 | 0.7118      | 0.7698      |\n",
    "\n",
    ": Metrics - AIZOO {#tbl-aizoo-metrics}\n",
    "\n",
    "![AIZOO Inference](final_report_images/aizoo/aizoo_metrics.png){#fig-aizoo}\n",
    "\n",
    "AIZOO Metrics\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8723d1-ef74-407b-bbb0-fbe307a34697",
   "metadata": {},
   "source": [
    "## Xception vs AIZOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59ceff-2b8a-45db-b3e3-6c7d16b16274",
   "metadata": {},
   "source": [
    "![AIZOO Inference](final_report_images/aizoo/aizoo_vs_xception.png){#fig-aizoo-xception width=70%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5bab73-ec5a-47ec-bd06-5ccda441e152",
   "metadata": {},
   "source": [
    "# Conclusion - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec50960-5050-41cc-b594-dfe80fd697ec",
   "metadata": {},
   "source": [
    "* Summary of the main findings and their implications\n",
    "* Limitations of the study and suggestions for future research\n",
    "* Suggestions for future research and improvements to the approach\n",
    "* Discussion of the potential impact of deep learning models on public health policy and compliance monitoring"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "mask-detection",
   "language": "python",
   "name": "mask-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
