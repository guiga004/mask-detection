{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b52af9f3-56f6-42dc-807e-d2c2200f3753",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Deep Learning Models for Measuring Mask-Wearing Behavior in Public Spaces\"\n",
    "author: \"Mohammed Guiga, M.S. Data Science\"\n",
    "bibliography: finalreport.bib\n",
    "notes-after-punctuation: true\n",
    "link-citations: true\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-line-numbers: true\n",
    "  pdf:\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "    number-sections: true\n",
    "    colorlinks: true\n",
    "    cite-method: biblatex\n",
    "    default-image-extension: tex\n",
    "    mainfont: EBGaramond-Regular\n",
    "    monofont: EBGaramond-Regular\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734eafbb-80aa-4662-ad13-eb693ab82a3d",
   "metadata": {},
   "source": [
    "{{< pagebreak >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257413d-7b21-4c35-9812-ed3544dd55e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea436e2-96a2-4d5e-bd3a-6480ae6c1915",
   "metadata": {},
   "source": [
    "I would like to acknowledge and express my gratitude to my capstone advisor, Dr. Julian Wolfson, for his guidance and patience throughout the entirety of this project. His expertise and insight were invaluable in shaping the direction of this research. \n",
    "\n",
    "I would also like to give a special thanks to my wife, Aminah Qandeel, for her unwavering support throughout the last two years. I couldn't have accomplished this without her!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce949e-2a05-485d-a3f5-081cf38e373c",
   "metadata": {},
   "source": [
    "{{< pagebreak >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd97f4-6c4e-4b65-9fc9-d4cc1cd21945",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Abstract {#sec-abstract}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9808376-0374-4936-b2e8-17de0d43c4d8",
   "metadata": {},
   "source": [
    "This project explores the feasibility of off-the-shelf deep learning models on a novel trail dataset. This dataset was curated by installing cameras at multiple locations adjacent to trails at several lakes in Minneapolis. The results of this project show the successful usage of off-the-shelf models for face detection, mask detection, and people tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea809047-9b03-4cf4-8e42-e01c89afa146",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction {#sec-introduction}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b5c1f-b6eb-4657-a415-68a1b0fb3a9a",
   "metadata": {},
   "source": [
    "Advances in deep learning have brought the technology to a point of maturity where many pre-trained models now exist for common tasks, such as object detection. As a result, smaller companies and industries that may have previously lacked the resources to invest in a machine learning department now can leverage this technology for their benefit. To explore this further, this project examines the performance of multiple off-the-shelf models on a novel dataset. \n",
    "\n",
    "Throughout this paper, the novel dataset explored in this project will be referred to as the \"novel trail dataset.\" This dataset was curated by installing ZED cameras at multiple locations adjacent to trails at several lakes in Minneapolis. These lakes encompass Lake of the Isles, Bde Maka Ska, and Lake Harriet. In total, 31 hours of footage in favorable weather was recorded, and 3 minutes, or 837 frames, of that footage was manually labeled to assess the performance of a mask detector. \n",
    "\n",
    "Four off-the-shelf deep learning models were tested on this dataset. One model for face detection, two for mask detection, and one for people tracking. The two mask detection models were able to be compared against each other since ground truth values were available for a subset of the data; sensitivity, specificity, precision, and accuracy were used as metrics to assess model performance. The other two models, face detection and people tracking were assessed qualitatively.\n",
    "\n",
    "The results of this work demonstrate the successful usage of off-the-shelf deep learning models for face detection, mask detection, and people tracking in a never-before-seen park setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b3ccd-c2f6-4c5b-a8c0-ae5c26f9b298",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Methods {#sec-methods}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2e9b-7ac8-4e79-b7fb-69aff036327d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Research Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18234aa4-1cb6-4866-a2d4-7ef980b11d16",
   "metadata": {},
   "source": [
    "The original research goal of this project, which began two years ago, was to test the efficacy of 2D/3D image-based sensors to quantify COVID compliance behaviors, namely movement behavior and exposure risk among public trail users. This involved quantifying physical distancing and mask-wearing behaviors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cbed8-593e-4614-89f8-0af2be3992c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cec0b7-db07-417b-a0c9-b3ee261e315e",
   "metadata": {},
   "source": [
    "ZED cameras were installed at multiple locations adjacent to trails at Lake of the Isles, Bde Maka Ska, and Lake Harriet. These locations were selected to be near amenities such as beaches, food/entertainment, and parks. MPRB permits were issued through the summer of 2021, and 31 hours of footage in favorable weather conditions were recorded. This footage was recorded on May 26, June 14, and June 18, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67798bf7-9d79-482b-bbbc-445e1b89e3da",
   "metadata": {},
   "source": [
    "The location of data collection sites are shown in @fig-lakes, examples of these sites are shown in @fig-trails, and the camera installation locations are shown in @fig-cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea3840-879b-481b-af0d-452bbbf2b657",
   "metadata": {},
   "source": [
    "In order to assess the performance of a mask classifier, a subset of footage needs to be labeled. Having access to the ground-truth values allows classifier metrics to be calculated as well as opening up the possibility of fine-tuning. Manual labeling was employed to achieve ground-truth labeling, and faces in 977 frames were labeled with their corresponding mask-wearing status. Of these 977 frames, 530 held the label \"No Mask\", and 447 \"Mask.\" Because the rate of mask-wearing by trail users was low, research staff conducted walk-bys while wearing masks to increase the amount of mask-wearing footage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22906728-ee9d-4b21-a4a3-e60ab35b721f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e25b57-7953-4d65-b34f-6c6f11ef7d59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e23ee-0b1f-42ea-b1a6-294c012fb088",
   "metadata": {},
   "source": [
    "Determining whether a person is wearing a face mask is dependent on being able to detect their face. Two mask detectors are evaluated in this work, and both utilize face detectors that are similar to a model called RetinaFace. The sections below briefly introduce facial detection frameworks explored in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253735f1-18d9-43d8-816b-f58266f17f3a",
   "metadata": {},
   "source": [
    "#### DeepFace and RetinaFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e308e59-6a8a-4c72-8985-2ab94da6baaf",
   "metadata": {},
   "source": [
    "DeepFace is an off-the-shelf facial recognition framework for python. This library includes access to state-of-the-art models such as VGG-Face, Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib, and SFace. RetinaFace is an off-the-shelf face detection package that utilizes a Resnet50 backbone. Resnet50 is a convolutional neural network-based model that consists of 48 convolution layers [@mascarenhas2021comparison]. Furthermore, the ResNet architecture allowed deep neural networks to mitigate greater training error percentages by introducing the addition of shortcut connections and residual functions [@mascarenhas2021comparison]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b5bbb-4cf0-4849-977a-9975c19bc8a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7422e9a-ca91-435a-b967-976bff04b1cc",
   "metadata": {},
   "source": [
    "#### Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96d237-20ce-4c68-a823-92bd6aaa1653",
   "metadata": {},
   "source": [
    "Xception is convolutional neural network architecture based entirely on depthwise separable convolution layers [@chollet2017xception]. A mask detection model was created by training and fine-tuning the Xception architecture on a mask-wearing dataset. This work was completed in Helena Sheild's Masters thesis: \"Masked Faces in Context (MASON) for Masked Face Detection and Classification.\" For the remainder of this paper, the fine-tuned Xception model will be referred to as \"Xception\", for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331d81e-05b5-4032-a8c5-19bd71cc97fd",
   "metadata": {},
   "source": [
    "This model was fine-tuned on a new dataset consisting of real-world images collected from image search sites. The goal of this model was to predict mask-wearing behavior. However, this model was not fine-tuned on the images collected from the novel trail dataset explored in this project, so it acted as a pure off-the-shelf model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b22da-4b86-433b-a5d4-0a0319232b8c",
   "metadata": {},
   "source": [
    "The Xception mask classifier architecture works as follows:\n",
    "\n",
    "1. Apply the RetinaNet face detection model to generate face crops.\n",
    "2. Feed cropped face to Xception model for mask classification.\n",
    "3. Final output is the face detected by RetinaFace along with the mask prediction.\n",
    "\n",
    "This architecture uses a pre-trained face detection model called RetinaNet, which was chosen due to its superior performance against other algorithms such as MT-CNN, Haar Cascade, and HOG. RetinaNet achieved higher recall and precision than the aforementioned algorithms. \n",
    "\n",
    "The Xception model is used in the mask detection portion of this architecture. Additionally, transfer learning was employed to use the existing model weights of the Xception model, and additional learning was completed iteratively to finetune against the newly gathered training set. \n",
    "\n",
    "Prior to testing on the novel trail dataset, this classifier was evaluated based on the F1 score metric and the Loss. It was demonstrated to achieve an F1-score of 0.99, and a loss of 0.0182. These metrics were evaluated on the dataset it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d06c0-1a8a-475a-bfcb-7eba90540fc9",
   "metadata": {},
   "source": [
    "#### AIZOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0339557-2502-49ba-882e-303714f4f941",
   "metadata": {},
   "source": [
    "The AIZOOtech group provides an off-the-shelf mask detection package which utilizes the structure of SSD (Single Shot MultiBox Detector). SSD is a method for detecting objects in images using a single deep neural network [@liu2016ssd]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ff136-ab49-4187-85be-1c7232654d9b",
   "metadata": {},
   "source": [
    "This architecture was chosen from several pre-trained models and was evaluated against the novel trail dataset and the previously introduced Xception model. This architecture is a pure off-the-shelf solution and was shown to have promise when confronted with unseen real-world data. The model tested is a FaceMask detection model developed by a group called AIZOOtech. This model was trained on 7971 images composed of WIDER FACE and MAFA. WIDER FACE is a face detection benchmark dataset that contains images from the publicly available WIDER dataset. This dataset contains images with a high degree of variability in scale, pose, and occlusion. Furthermore, this variability can help deter the model from overfitting during training. AIZOO provides API access to their model via PyTorch, TensorFlow, Keras, MXNet, Caffee, Paddle, and OpenCV dnn. The PyTorch framework was used for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56136d1-07ef-4dc6-83ce-0a71ab31ca56",
   "metadata": {},
   "source": [
    "#### Metric Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac37eb-a764-45fe-9727-affe42e1b3f6",
   "metadata": {},
   "source": [
    "Standard metrics were used to evaluate the models introduced above. Precision and accuracy were chosen since they are very common metrics used across the fields of computer science and machine learning. In addition, we also chose to use sensitivity and specificity. All four metrics were assessed across models and sub-groups, but the focal point was sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f099641-3b34-42a7-904b-e6745d7285e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### People Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd306dd-de1a-4e74-ba19-0076a42d8484",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Trackformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617fc54-937c-406c-971f-bc448c340b79",
   "metadata": {},
   "source": [
    "The task of multi-object tracking (MOT) presents a challenge that requires simultaneous reasoning about track initialization, identity, and spatiotemporal trajectories. In a paper titled “TrackFormer: Multi-Object Tracking with Transformers”, the authors introduce an end-to-end trainable MOT approach called TrackFormer. TrackFormer is based on an encoder-decoder Transformer architecture and was shown to achieve state-of-the-art performance on MOT17 and MOTS20 without relying on any additional graph optimization, or modeling of motion and/or appearance [@meinhardt2022trackformer].\n",
    "\n",
    "The network uses a CNN (ResNet-50) to identify features in the frame that get passed through the Transformer Encoder and Decoder to identify objects to track. When objects are identified, track queries are initiated that get passed onto the next frame in the sequence. The track queries from previous frames are passed to the Transformer Decoder to inform target recognition and maintain a memory of the objects in the scene to track them over time.\n",
    "\n",
    "The primary data set used to train and evaluate the original model was MOT17 along with further investigation using MOT20 and MOTS20. The data includes video clips of multiple humans moving throughout the frame in different environments with varying levels of complexity and crowdedness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ebe8e-98db-4997-a92b-0b960c4bb9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results {#sec-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550c4a2-70a3-405c-8e75-df484128d472",
   "metadata": {},
   "source": [
    "This section covers the results of the different models. The mask detection models were applied to the labeled frames and the predicted results were compared to the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eba5ed-c1b7-4457-851d-24f8098af77e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7f981-b8ae-4add-ba5b-23508ef60ae9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DeepFace {#sec-deepface-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ddc71-3d1c-4ea9-8157-53489c5cf1e7",
   "metadata": {},
   "source": [
    "The Xception architecture typically only picks up one face in each frame. However, the DeepFace model, using the retinaface backend, successfully finds multiple faces. An example of this can be seen in @fig-deepface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d6038-9e7a-47ed-98d6-8b0b02b1792e",
   "metadata": {},
   "source": [
    "### RetinaFace {#sec-retinaface-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d4a54-5721-474a-b572-5ce068f84bcc",
   "metadata": {},
   "source": [
    "The retinaface backend proved successful when testing the DeepFace framework. Retinaface also provides the capability to extract facial landmarks, such as the eyes, nose, and mouth corners. One hypothesis was to use the facial landmarks, or lack-there-of, to infer whether or not a mask is being worn. However, as can be seen in @fig-retina-mask2, the RetinaFace algorithm attempts to find facial landmark points regardless of mask-wearing behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b17040-3ec6-4339-bcb7-3a3b1263d47f",
   "metadata": {},
   "source": [
    "Moreover, successful facial landmark detection opens up additional avenues of analysis such as head pose, gaze direction, and facial expression estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3fbd6-6d42-4667-bc44-9ee33222bad2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62988b9b-bb38-4e6b-a64e-36e199c5a8e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9308f95-7afc-41f0-a673-f9647a323152",
   "metadata": {},
   "source": [
    "The Xception model was evaluated against standard metrics. The results can be seen in @tbl-metrics-full-dataset and plots can be found in @fig-all-groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dd813-2bac-4b79-a735-bed9f736fc3c",
   "metadata": {},
   "source": [
    "::: {#fig-metrics_full_dataset layout=\"[50, 50]\"}\n",
    "\n",
    "|    Metric   |  Score | Lower  |  Upper  |\n",
    "|:-----------:|:------:|:------:|:-------:|\n",
    "| Sensitivity | 0.799 | 0.761 | 0.840  |\n",
    "| Specificity | 0.430 | 0.392 | 0.468  |\n",
    "| Precision   | 0.542 | 0.509 | 0.586  |\n",
    "| Accuracy    | 0.599 | 0.565 | 0.623  |\n",
    "\n",
    ": Metrics - full dataset {#tbl-metrics-full-dataset}\n",
    "\n",
    "![Metrics with Confidence Intervals](final_report_images/metrics/all_groups_metrics.png){#fig-all-groups}\n",
    "\n",
    "Xception Model Metrics\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f764d-315d-4ca4-a460-0ff332022797",
   "metadata": {},
   "source": [
    "The lower bound and upper bounds of the metrics were calculated by bootstrapping a 95% confidence interval. Bootstrap methods, in general, consist of estimating a characteristic of the unknown population by simulating the characteristic when the true population is replaced by an estimated one [see @diciccio1988review]. The 95th percentile method was used to calculate a set of approximate confidence limits for the metrics defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312513b5-685a-4164-b8d5-deeb59fce65c",
   "metadata": {},
   "source": [
    "The metric results for the full dataset provided some insights into the limitations of the trained model. A noteworthy limitation is a specificity equal to 0.4 and a sensitivity equal to 0.8. The relatively high sensitivity paired with a low specificity shows that this model overpredicts mask-wearing. Additional sub-groups were analyzed to see if the model performed better in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8bfe-fa1b-47a2-9099-aec2431dc34f",
   "metadata": {},
   "source": [
    "#### Sub-group Metric Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819366d-20bd-44fd-9e72-321501387930",
   "metadata": {},
   "source": [
    "See @fig-quadrants for metrics calculated across frame quadrants. The first and third quadrants represent the upper-left and upper-right quadrants of each frame, respectively. Quadrants 2 and 4 represent the bottom two quadrants of each frame, and in most cases are images of the ground. In this analysis, all faces were found in quadrants 1 and 3. Both quadrant 1 and quadrant 3 have poor specificity, with quadrant 3 having very poor specificity compared to the entire dataset. Both of these quadrants have high sensitivity also, which indicates overprediction independent of the quadrant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb4357-c9d0-4c49-8a4b-b79172e4b735",
   "metadata": {},
   "source": [
    "Metrics were also calculated across a combination of quadrants and dates. The hypothesis was that since both the quadrants and dates yielded similar results, the combination will yield overprediction as well. Each sub-group was shown to have high sensitivity and low specificity, which aligned with the overprediction hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c7bb6-642c-4335-a168-c6886a6aca6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Image Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6e534-5748-4b25-b919-820fd5da2760",
   "metadata": {},
   "source": [
    "Quadrant 1 image examples can be seen in @fig-q1, and quadrant 3 image examples can be seen in @fig-q3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e4688-6a14-4693-8dd5-5fc42c4525d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AIZOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0877f-cdd7-4a7a-90fe-d06f78d5eaec",
   "metadata": {},
   "source": [
    "The AIZOO model was ran on the trail dataset and metrics were calculated. This model was ran through the PyTorch interface, and was able to achieve a precision of 0.96. See @fig-aizoo-metrics for full results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76decc-cbca-4770-8663-e2ce607459ea",
   "metadata": {},
   "source": [
    "::: {#fig-aizoo-metrics layout=\"[50, 50]\"}\n",
    "\n",
    "|    Metric   |  Score | Lower    |  Upper  |\n",
    "|:-----------:|:------:|:--------:|:-------:|\n",
    "| Sensitivity | 0.520 | 0.478   | 0.574  |\n",
    "| Specificity | 0.980 | 0.964   | 0.993  |\n",
    "| Precision   | 0.968 | 0.933   | 0.986  |\n",
    "| Accuracy    | 0.739 | 0.712   | 0.770  |\n",
    "\n",
    ": Metrics - AIZOO {#tbl-aizoo-metrics}\n",
    "\n",
    "![AIZOO Inference](final_report_images/aizoo/aizoo_metrics.png){#fig-aizoo-metrics}\n",
    "\n",
    "AIZOO Metrics\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bdc1a-3b55-41cc-9c06-97a60673b4b3",
   "metadata": {},
   "source": [
    "The AIZOO model was able to achieve a specificity of 0.98. This means that the model is very unlikely to generate false positives, meaning that mask predictions are more reliable and can be trusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8723d1-ef74-407b-bbb0-fbe307a34697",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Xception vs AIZOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffb172-5f34-4623-99ac-3207ebf34b99",
   "metadata": {},
   "source": [
    "The performance of the Xception model and the AIZOO model were compared. See @fig-aizoo-xception for full results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb9e4-15f3-47c2-a0a8-f5ed473dbe93",
   "metadata": {},
   "source": [
    "![Xception vs AIZOO](final_report_images/aizoo/aizoo_vs_xception.png){#fig-aizoo-xception width=50%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e72786-cef9-4a53-8836-75b1cae13e1b",
   "metadata": {},
   "source": [
    "AIZOO had lower sensitivity but higher specificity, leading to substantially higher precision. This was consistent across all datasets and showed that the AIZOO model provides more reliable mask predictions than the Xception model. The accuracy for the AIZOO model is 0.73, which isn't optimal. However, the high precision score is a great starting ground for further fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59114a20-dfa1-45e1-a304-65885512f6b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## People Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a62f4-5bee-4b19-abc0-8f8048591d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trackformer {#sec-trackformer-results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473ff72-2490-48ce-9e79-b570e24ad658",
   "metadata": {},
   "source": [
    "TrackFormer was able to successfully track people across multiple frames by utlizing an encoder-decoder transformer architecture. See @fig-trackformer for an example of the model tracking people, and a dog, throughout frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a8a31-5683-4d9d-bf1e-003169588ef6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Limitations {#sec-limitations}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cab55-4b70-4857-af70-fefe10f55de1",
   "metadata": {},
   "source": [
    "## Unrepresentative Dataset for Mask Wearing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bac5fb-1636-4911-aad4-472cc30b51a2",
   "metadata": {},
   "source": [
    "The lack of mask-wearing footage led to an unrepresentative dataset for mask-wearing. MPRB permits were received after mask mandates were lifted, requiring the team to heavily rely on UMN actors for mask-wearing footage. After mask mandates were lifted in Minnesota it was uncommon to see someone wearing a mask outdoors. This led the research team to plant mask-wearers on the trails to capture footage. Analyzing the footage made it clear that the actors did not imitate a traditional trail goer; instead of traversing the trail linearly: out of the frame from afar, towards the camera, and out of the frame again, the actors paced around and sat in front of the camera. Furthermore, the number of mask-wearing actors was very small, leading to a disproportionate amount of frames containing actors. This led to an unrepresentative dataset for mask-wearing, which made it difficult to assess how well the mask detectors would perform on this type of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282bc06-dac9-43b0-89b8-3b0d862f0ff1",
   "metadata": {},
   "source": [
    "## Difficult to Employ Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f549a4-2911-4425-8974-e868f72b529d",
   "metadata": {},
   "source": [
    "The limited number of labeled images made it difficult to employ transfer learning. Transfer learning involves taking a model with pre-trained weights, and applying additional rounds of training with a new dataset. This is commonly referred to as \"fine-tuning\", and can help with the performance of a model. Out of 31 hours of footage, 977 frames (about 3 minutes) were labeled. Furthermore, out of these 977 frames, 447 had a positive mask label. The majority of these frames contained one mask-wearing actor, and fine-tuning could have led to an overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de6412-b191-4ddf-9465-e525dabefbb7",
   "metadata": {},
   "source": [
    "## High Proportion of Low-Confidence Mask Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa05ba-979e-4f85-aed7-8d41ad49e7eb",
   "metadata": {},
   "source": [
    "The high proportion of small faces, relative to the frame, led to a high proportion of faces not yielding adequate confidence to enable a mask prediction. The camera's positioning was angled in a way that captured more footage of people further away from the camera than close to it. Analyzing the prediction results, it was determined that faces closer to the camera (relatively bigger, higher resolution) yielded higher confidence predictions. One way to address this limitation is to find a camera position that captures more up-close footage. Another way to expand this study, which doesn't require redesigning the experiment, is to include tracking, which could potentially improve the accuracy of predictions. Tracking people throughout frames would allow the higher confidence predictions to play a bigger role in determining the mask-wearing status of each person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5bab73-ec5a-47ec-bd06-5ccda441e152",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion {#sec-conclusion}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e4e3b-f7f2-4a9d-9205-809544ab57c0",
   "metadata": {},
   "source": [
    "Off-the-shelf deep learning models were successfully used for face detection, mask detection, and people tracking on a novel trail dataset. These models included RetinaFace for face detection, Xception and AIZOO-based models for mask detection, and Trackformer for people tracking. Limitations such as a lack of labeled images and substandard camera positioning were highlighted, and ways to mitigate these limitations were discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a51461-f2f9-4d1f-9d40-b6f854bfc914",
   "metadata": {},
   "source": [
    "{{< pagebreak >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5da3a5-a400-42eb-90a8-6e8a18e6545a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "::: {#refs}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabe170-4cc7-41df-bdde-15dc2cecad6b",
   "metadata": {},
   "source": [
    "{{< pagebreak >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339bbaeb-9e94-4b3a-a337-488839211e43",
   "metadata": {},
   "source": [
    "# Appendix {.appendix}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad29a7d1-8d33-4b69-b60a-b9f22977080d",
   "metadata": {},
   "source": [
    "::: {#fig-lakes layout=\"[-5, 20, -5, 20, -5, 20, -5]\"}\n",
    "\n",
    "![Lake of the Isles](final_report_images/camera/lake_of_the_isles.png){#fig-isles}\n",
    "\n",
    "![Bde Maka Ska](final_report_images/camera/bde_maka_ska.png){#fig-maka}\n",
    "\n",
    "![Lake Harriet](final_report_images/camera/lake_harriet.png){#fig-harriet}\n",
    "\n",
    "Data Collection Locations\n",
    ":::"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05b31216-a5ff-4e5a-a1ec-cee5377c6c7e",
   "metadata": {},
   "source": [
    "::: {#fig-trails layout=\"[-5, 20, -5, 20, -5, 20, -5]\"}\n",
    "\n",
    "![Lake of the Isles](final_report_images/camera/lake_of_the_isles_img.jpg){#fig-isles}\n",
    "\n",
    "![Bde Maka Ska](final_report_images/camera/bde_maka_ska_img.jpg){#fig-maka}\n",
    "\n",
    "![Lake Harriet](final_report_images/camera/lake_harriet_img.jpg){#fig-harriet}\n",
    "\n",
    "Data Collection Locations Examples\n",
    ":::"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4053f47-4cc1-4d7c-a154-c362e4de2a10",
   "metadata": {},
   "source": [
    "::: {#fig-cameras layout=\"[-5, 20, -5, 20, -5, 20, -5]\"}\n",
    "\n",
    "![Lake of the Isles](final_report_images/camera/lake_of_the_isles_camera.png){#fig-isles}\n",
    "\n",
    "![Bde Maka Ska](final_report_images/camera/bde_maka_ska_camera.png){#fig-maka}\n",
    "\n",
    "![Lake Harriet](final_report_images/camera/lake_harriet_camera.png){#fig-harriet}\n",
    "\n",
    "Data Collection Camera Positions\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c9a652-9e01-41a9-9164-c7e10b9c3e68",
   "metadata": {},
   "source": [
    "::: {#fig-deepface layout=\"[30, -5, 30, -5, 30]\"}\n",
    "\n",
    "![Original Image](final_report_images/deepface/og.png){#fig-deepface-og}\n",
    "\n",
    "![Xception](final_report_images/deepface/xception.png){#fig-deepface-xception}\n",
    "\n",
    "![DeepFace](final_report_images/deepface/deepface.png){#fig-deepface-deepface}\n",
    "\n",
    "Facial Detection\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ed4d4-c35e-4e42-aa8a-0d73984ccbca",
   "metadata": {},
   "source": [
    "::: {#fig-retinaface layout=\"[[-25, 50, -25], [-10, 40, 40, -10]]\"}\n",
    "\n",
    "![Facial Landmark Extraction](final_report_images/retinaface/retinaface_2.png){#fig-retina2}\n",
    "\n",
    "![Original Image](final_report_images/retinaface/retinaface_mask_1.png){#fig-retina-mask1}\n",
    "\n",
    "![Facial Landmark Extraction](final_report_images/retinaface/retinaface_mask_2.png){#fig-retina-mask2}\n",
    "\n",
    "Facial Landmark Detection\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783de1c0-6733-41a8-bfe7-dfe75d0a9523",
   "metadata": {},
   "source": [
    "![Metrics Across Quadrants](final_report_images/metrics/quadrants_metrics.png){#fig-quadrants width=50%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381420c-ea32-4bac-86d5-8dff08185acb",
   "metadata": {},
   "source": [
    "::: {#fig-q1 layout=\"[-20, 30, 30, -20]\"}\n",
    "\n",
    "![](final_report_images/examples/q1/q1_1.png)\n",
    "\n",
    "![](final_report_images/examples/q1/q1_2.png)\n",
    "\n",
    "Quadrant 1 Examples\n",
    ":::\n",
    "\n",
    "::: {#fig-q3 layout=\"[-20, 30, 30, -20]\"}\n",
    "\n",
    "![](final_report_images/examples/q3/q3_1.png)\n",
    "\n",
    "![](final_report_images/examples/q3/q3_2.png)\n",
    "\n",
    "Quadrant 3 Examples\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa90dba-a375-4775-9e51-efbb4de0efb4",
   "metadata": {},
   "source": [
    "![AIZOO Inference Example](final_report_images/aizoo/aizoo.png){#fig-aizoo-example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a14a52-62bf-4cf4-8dce-fe97f6532631",
   "metadata": {},
   "source": [
    "![Trackformer Example](final_report_images/trackformer/trackformer.png){#fig-trackformer}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "mask-detection",
   "language": "python",
   "name": "mask-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
